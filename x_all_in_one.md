# üìö Information Retrieval Lab - Viva Questions & Answers
## Table of Contents

---

## üìñ Overview
- [Introduction](#introduction)
- [Document Summary](#document-summary)
- [How to Use This Guide](#how-to-use-this-guide)

---

## üìå [Assignment 1: Information Retrieval Basics](#assignment-1-information-retrieval-basics)

### Frequently Asked Questions
- [Q1. What is meant by Information Retrieval?](#q1-what-is-meant-by-information-retrieval)
- [Q2. Draw the diagram of IR?](#q2-draw-the-diagram-of-ir)
- [Q3. Difference between DR and IR](#q3-difference-between-dr-and-ir)
- [Q4. Explain Conflation Algorithm](#q4-explain-conflation-algorithm)
- [Q5. What is the necessity of feedback mechanism in IR?](#q5-what-is-the-necessity-of-feedback-mechanism-in-ir)
- [Q6. What is document representative?](#q6-what-is-document-representative)
- [Q7. List out some high frequency words and why we called them high frequency words?](#q7-list-out-some-high-frequency-words-and-why-we-called-them-high-frequency-words)
- [Q8. If input string = "some text is here" then what will be the output of Conflation Algorithm?](#q8-if-input-string--some-text-is-here-then-what-will-be-the-output-of-conflation-algorithm)

### Questions Related to Applications of IR
- [Q1. If your search is based on word which will occur very rarely in the document then what will happen?](#q1-if-your-search-is-based-on-word-which-will-occur-very-rarely-in-the-document-then-what-will-happen)

---

## üìå [Assignment 2: Clustering in Information Retrieval](#assignment-2-clustering-in-information-retrieval)

### Frequently Asked Questions
- [Q1. What do you mean by cluster? What is cluster Seed?](#q1-what-do-you-mean-by-cluster-what-is-cluster-seed)
- [Q2. Why we need cluster in IR? What is document based and Term based Clustering?](#q2-why-we-need-cluster-in-ir-what-is-document-based-and-term-based-clustering)
- [Q3. How can we define relevance between documents?](#q3-how-can-we-define-relevance-between-documents)
- [Q4. How can we form the cluster of documents?](#q4-how-can-we-form-the-cluster-of-documents)
- [Q5. How many methods are there to define cluster?](#q5-how-many-methods-are-there-to-define-cluster)
- [Q6. Explain Single Pass Algorithm. What do you mean by overlapping and exclusive approach?](#q6-explain-single-pass-algorithm-what-do-you-mean-by-overlapping-and-exclusive-approach)
- [Q7. What is meant by similarity matrix?](#q7-what-is-meant-by-similarity-matrix)
- [Q8. What is the time complexity and space complexity for Single Pass?](#q8-what-is-the-time-complexity-and-space-complexity-for-single-pass)
- [Q9. Which method is better for cluster definition?](#q9-which-method-is-better-for-cluster-definition)

### Questions Related to Applications of Clustering
- [Q1. Illustrate the situation when clustering will not be possible?](#q1-illustrate-the-situation-when-clustering-will-not-be-possible)
- [Q2. Create the document based clustering for 5 documents.](#q2-create-the-document-based-clustering-for-5-documents)

---

## üìå [Assignment 3: Inverted Index](#assignment-3-inverted-index)

### Frequently Asked Questions
- [Q1. What do you mean by inverted file?](#q1-what-do-you-mean-by-inverted-file)
- [Q2. What is word oriented mechanism?](#q2-what-is-word-oriented-mechanism)
- [Q3. What do you mean by dictionary?](#q3-what-do-you-mean-by-dictionary)
- [Q4. What is the role of postings?](#q4-what-is-the-role-of-postings)
- [Q5. What are different applications of inverted index?](#q5-what-are-different-applications-of-inverted-index)
- [Q6. How inverted index is helpful in searching?](#q6-how-inverted-index-is-helpful-in-searching)
- [Q7. What do you mean by merging similar terms from different documents?](#q7-what-do-you-mean-by-merging-similar-terms-from-different-documents)
- [Q8. What is the time complexity and space complexity for inverted index?](#q8-what-is-the-time-complexity-and-space-complexity-for-inverted-index)
- [Q9. What are applications of inverted file?](#q9-what-are-applications-of-inverted-file)

### Questions Related to Applications
- [Q1. Illustrate the situation when inverted file will not be effective as storage option?](#q1-illustrate-the-situation-when-inverted-file-will-not-be-effective-as-storage-option)

---

## üìå [Assignment 4: Precision and Recall](#assignment-4-precision-and-recall)

### Frequently Asked Questions
- [Q1. What do you mean by Precision?](#q1-what-do-you-mean-by-precision)
- [Q2. What do you mean by Recall?](#q2-what-do-you-mean-by-recall)
- [Q3. Explain the need of performance evaluation of IR system](#q3-explain-the-need-of-performance-evaluation-of-ir-system)
- [Q4. Explain how to evaluate performance of IR system.](#q4-explain-how-to-evaluate-performance-of-ir-system)

---

## üìå [Assignment 5: F-Measure and E-Measure](#assignment-5-f-measure-and-e-measure)

### Frequently Asked Questions
- [Q1. What do you mean by F Measure?](#q1-what-do-you-mean-by-f-measure)
- [Q2. What do you mean by E Measure?](#q2-what-do-you-mean-by-e-measure)
- [Q3. What is the Significance of F and E measure in IR system?](#q3-what-is-the-significance-of-f-and-e-measure-in-ir-system)

---

## üìå [Assignment 6: Image Processing & Histograms](#assignment-6-image-processing--histograms)

### Frequently Asked Questions
- [Q1. What do you mean by image histogram?](#q1-what-do-you-mean-by-image-histogram)
- [Q2. What are different types of file format for images?](#q2-what-are-different-types-of-file-format-for-images)
- [Q3. Explain the role of color model](#q3-explain-the-role-of-color-model)
- [Q4. Compare different color models.](#q4-compare-different-color-models)
- [Q5. Define image histogram?](#q5-define-image-histogram)
- [Q6. List types of histogram](#q6-list-types-of-histogram)
- [Q7. Explain use of image histogram](#q7-explain-use-of-image-histogram)
- [Q8. How to extract R, G and B content from image](#q8-how-to-extract-r-g-and-b-content-from-image)

### Questions Related to Applications
- [Q1. Consider the image processing domain and explain various methods of extracting intensity of pixel](#q1-consider-the-image-processing-domain-and-explain-various-methods-of-extracting-intensity-of-pixel)

---

## üìå [Assignment 7: Web Crawlers](#assignment-7-web-crawlers)

### Frequently Asked Questions
- [Q1. What is meant by crawler?](#q1-what-is-meant-by-crawler)
- [Q2. Explain working of web crawler](#q2-explain-working-of-web-crawler)
- [Q3. Give examples of Web Crawlers.](#q3-give-examples-of-web-crawlers)
- [Q4. Specify role of crawler in searching](#q4-specify-role-of-crawler-in-searching)
- [Q5. Explain Meta Crawler](#q5-explain-meta-crawler)

### Questions Related to Applications
- [Q1. Modify developed WebCrawler considering other parameters or keywords](#q1-modify-developed-webcrawler-considering-other-parameters-or-keywords)

---

## üìå [Assignment 8: Weather API Integration](#assignment-8-weather-api-integration)

### Frequently Asked Questions
- [Q1. Explain the steps to implement this assignment?](#q1-explain-the-steps-to-implement-this-assignment)
- [Q2. What is OpenWeatherMap service?](#q2-what-is-openweathermap-service)
- [Q3. What is the use of JSON in this assignment?](#q3-what-is-the-use-of-json-in-this-assignment)

### Questions Related to Applications
- [Q1. How to use this python code to predict the weather for next few days?](#q1-how-to-use-this-python-code-to-predict-the-weather-for-next-few-days)

---

## üìå [Assignment 9: Recommender Systems](#assignment-9-recommender-systems)

### Frequently Asked Questions
- [Q1. Define recommender system](#q1-define-recommender-system)
- [Q2. List types of recommender system.](#q2-list-types-of-recommender-system)
- [Q3. Explain how the Recommender systems reduce information overload by estimating Relevance?](#q3-explain-how-the-recommender-systems-reduce-information-overload-by-estimating-relevance)
- [Q4. Explain Collaborative filtering](#q4-explain-collaborative-filtering)
- [Q5. Explain Hybrid Recommender system](#q5-explain-hybrid-recommender-system)

### Questions Related to Applications
- [Q1. If you want to buy a product from Amazon.in website. Explain how recommender system is helpful to enhance your buying product experience](#q1-if-you-want-to-buy-a-product-from-amazonin-website-explain-how-recommender-system-is-helpful-to-enhance-your-buying-product-experience)

---




# üìö Information Retrieval Lab - Viva Questions & Answers

---

## üìå Assignment 1: Information Retrieval Basics

### **Frequently Asked Questions**

#### **Q1. What is meant by Information Retrieval?**
**Answer:**  
Information Retrieval (IR) is the process of obtaining relevant information from a large collection of data resources based on user queries. It involves searching, retrieving, and presenting information that satisfies a known query where the user knows what they are looking for.

---

#### **Q2. Draw the diagram of IR?**
**Answer:**  
A typical IR system consists of:
- **User Query Input**
- **Query Processing Module**
- **Document Collection/Database**
- **Indexing & Retrieval Engine**
- **Ranking Module**
- **Result Presentation**

*(Note: Diagram should show the flow from query input ‚Üí processing ‚Üí matching ‚Üí ranking ‚Üí output)*

---

#### **Q3. Difference between DR and IR**
**Answer:**  
| Feature | Data Retrieval (DR) | Information Retrieval (IR) |
|---------|---------------------|---------------------------|
| **Query Type** | Structured queries (SQL) | Natural language queries |
| **Matching** | Exact match | Approximate/Relevance match |
| **Result** | All matching records | Ranked relevant documents |
| **Evaluation** | Precision-focused | Precision & Recall both important |

---

#### **Q4. Explain Conflation Algorithm**
**Answer:**  
Conflation Algorithm is used to reduce words to their root or base form by removing suffixes and prefixes. It groups related words together (e.g., "running," "runs," "ran" ‚Üí "run"). This process is also known as **stemming** and helps in improving search accuracy by matching different forms of the same word.

---

#### **Q5. What is the necessity of feedback mechanism in IR?**
**Answer:**  
Feedback mechanisms are necessary to:
- Improve search accuracy by learning from user interactions
- Refine query results based on relevance feedback
- Adapt the system to user preferences over time
- Reduce information overload by filtering irrelevant results

---

#### **Q6. What is document representative?**
**Answer:**  
A document representative is a concise representation of a document's content, typically consisting of:
- **Keywords** or significant terms
- **Term frequency** information
- **Document vectors** in vector space models
- **Metadata** (title, author, date)

This representation is used for indexing and matching documents with user queries.

---

#### **Q7. List out some high frequency words and why we called them high frequency words?**
**Answer:**  
High frequency words include: **"the," "is," "at," "of," "and," "a," "to," "in," "for," "on"**

These are called high frequency words (or **stop words**) because:
- They appear very frequently in documents
- They carry little semantic meaning
- They don't help distinguish between documents
- They are usually filtered out during indexing to improve efficiency

---

#### **Q8. If input string = "some text is here" then what will be the output of Conflation Algorithm?**
**Answer:**  
After applying conflation/stemming:
- "some" ‚Üí "some" (no change)
- "text" ‚Üí "text" (no change)
- "is" ‚Üí "is" (stop word, may be removed)
- "here" ‚Üí "here" (no change)

Final output: **"some text here"** (assuming stop words are removed)

---

### **Questions Related to Applications of IR**

#### **Q1. If your search is based on word which will occur very rarely in the document then what will happen?**
**Answer:**  
If the search term appears very rarely:
- **Higher significance** will be assigned to that term (high IDF - Inverse Document Frequency)
- Documents containing this rare term will be **ranked higher**
- The term is considered more **discriminative** and relevant
- Search results will be more **focused and specific**
- May result in **fewer but more relevant results**

---

## üìå Assignment 2: Clustering in Information Retrieval

### **Frequently Asked Questions**

#### **Q1. What do you mean by cluster? What is cluster Seed?**
**Answer:**  
- **Cluster:** A cluster is a group of similar documents or items grouped together based on common characteristics or similarity measures.
- **Cluster Seed:** The initial document or representative used as the starting point to form a cluster. Other similar documents are added around this seed document.

---

#### **Q2. Why we need cluster in IR? What is document based and Term based Clustering?**
**Answer:**  
**Need for Clustering:**
- Improves search efficiency by grouping similar documents
- Reduces search space
- Helps in document organization
- Improves retrieval accuracy

**Types:**
- **Document-based Clustering:** Groups documents based on their overall content similarity
- **Term-based Clustering:** Groups terms/keywords that frequently co-occur together

---

#### **Q3. How can we define relevance between documents?**
**Answer:**  
Relevance between documents can be defined using:
- **Cosine Similarity:** Measures angle between document vectors
- **Jaccard Coefficient:** Ratio of common terms to total unique terms
- **Euclidean Distance:** Distance between document vectors
- **Pearson Correlation:** Statistical similarity measure
- **Term overlap:** Number of common keywords

---

#### **Q4. How can we form the cluster of documents?**
**Answer:**  
Steps to form clusters:
1. **Calculate similarity** between all document pairs
2. **Select initial seeds** (cluster centers)
3. **Assign documents** to nearest cluster seed
4. **Update cluster representatives** based on assigned documents
5. **Iterate** until convergence or stopping criteria met
6. **Finalize clusters** with stable membership

---

#### **Q5. How many methods are there to define cluster?**
**Answer:**  
Main clustering methods:
1. **Hierarchical Clustering** (Agglomerative/Divisive)
2. **Partitioning Methods** (K-means, K-medoids)
3. **Density-based** (DBSCAN)
4. **Single Pass Algorithm**
5. **Complete Link Method**
6. **Single Link Method**
7. **Average Link Method**

---

#### **Q6. Explain Single Pass Algorithm. What do you mean by overlapping and exclusive approach?**
**Answer:**  
**Single Pass Algorithm:**
- Documents are processed sequentially in one pass
- Each document is compared with existing cluster representatives
- If similarity exceeds threshold, document joins that cluster
- Otherwise, a new cluster is created with that document as seed
- Simple and computationally efficient

**Approaches:**
- **Exclusive (Hard) Clustering:** Each document belongs to exactly one cluster
- **Overlapping (Soft) Clustering:** Documents can belong to multiple clusters with different membership degrees

---

#### **Q7. What is meant by similarity matrix?**
**Answer:**  
A similarity matrix is a square matrix where:
- **Rows and columns** represent documents
- **Cell values** represent similarity scores between document pairs
- **Diagonal elements** are typically 1 (document's similarity with itself)
- **Symmetric matrix** (similarity(A,B) = similarity(B,A))
- Used to visualize and compute document relationships

---

#### **Q8. What is the time complexity and space complexity for Single Pass?**
**Answer:**  
- **Time Complexity:** O(n√ók), where n = number of documents, k = number of clusters
- **Space Complexity:** O(n√óm), where n = number of documents, m = number of features/terms

Single Pass is relatively efficient compared to hierarchical methods.

---

#### **Q9. Which method is better for cluster definition?**
**Answer:**  
The choice depends on requirements:
- **K-means:** Better for well-separated, spherical clusters
- **Hierarchical:** Better when cluster hierarchy is important
- **DBSCAN:** Better for arbitrary-shaped clusters with noise
- **Single Pass:** Better for large datasets requiring efficiency

For IR systems, **Single Pass** and **K-means** are commonly preferred for their balance of accuracy and efficiency.

---

### **Questions Related to Applications of Clustering**

#### **Q1. Illustrate the situation when clustering will not be possible?**
**Answer:**  
Clustering may not be possible when:
- **No similarity exists** between any documents
- **All documents are identical** (no distinguishing features)
- **Extreme noise** in data makes patterns undetectable
- **High dimensionality** with sparse data (curse of dimensionality)
- **Single document** in the collection
- **Uniform distribution** where no natural groupings exist

---

#### **Q2. Create the document based clustering for 5 documents.**
**Answer:**  
Example with 5 documents:

**Documents:**
- D1: "machine learning artificial intelligence"
- D2: "data mining machine learning"
- D3: "web search engine crawler"
- D4: "information retrieval web search"
- D5: "deep learning neural networks"

**Clustering Process:**
1. Calculate similarity between documents
2. Form clusters based on similarity threshold

**Resulting Clusters:**
- **Cluster 1 (AI/ML):** D1, D2, D5 - Related to machine learning and AI
- **Cluster 2 (Web/IR):** D3, D4 - Related to web search and retrieval

---

## üìå Assignment 3: Inverted Index

### **Frequently Asked Questions**

#### **Q1. What do you mean by inverted file?**
**Answer:**  
An inverted file (inverted index) is a data structure that maps content (terms/words) to their locations in documents. Instead of storing documents with their terms, it stores terms with a list of documents containing them. It's called "inverted" because it reverses the typical document-to-term relationship.

---

#### **Q2. What is word oriented mechanism?**
**Answer:**  
Word-oriented mechanism is an indexing approach where:
- Each unique **word/term** is treated as an index key
- The system maintains pointers to all documents containing that word
- Enables fast **word-level searching**
- Allows **exact keyword matching**
- More granular than document-level indexing

---

#### **Q3. What do you mean by dictionary?**
**Answer:**  
In the context of inverted index, a dictionary is:
- A **vocabulary** of all unique terms in the document collection
- Stores terms in **sorted order** (usually alphabetically)
- Contains **metadata** about each term (frequency, IDF value)
- Acts as the **lookup table** to access postings lists
- Typically implemented using hash tables or B-trees for efficient access

---

#### **Q4. What is the role of postings?**
**Answer:**  
Postings (or postings list) serve to:
- Store the **list of documents** containing a specific term
- Include **position information** (which position in the document)
- Contain **term frequency** within each document
- Enable **phrase queries** through position data
- Support **Boolean retrieval** operations
- Facilitate **ranking** of search results

---

#### **Q5. What are different applications of inverted index?**
**Answer:**  
- **Search engines:** Fast keyword search (Google, Bing)
- **Database systems:** Full-text search in databases
- **E-commerce:** Product search functionality
- **Email clients:** Searching through emails
- **Document management systems:** Enterprise document search
- **Code repositories:** Searching source code (GitHub)
- **Digital libraries:** Academic paper search

---

#### **Q6. How inverted index is helpful in searching?**
**Answer:**  
Inverted index improves searching by:
- **Fast lookup:** O(1) or O(log n) term lookup in dictionary
- **Direct access:** Jump directly to documents containing the term
- **No sequential scanning:** Avoids reading all documents
- **Boolean operations:** Easy to perform AND, OR, NOT operations
- **Ranking support:** Quick access to term frequency data
- **Scalability:** Handles large document collections efficiently

---

#### **Q7. What do you mean by merging similar terms from different documents?**
**Answer:**  
Merging similar terms involves:
- **Combining postings lists** of the same term from different documents
- **Aggregating term occurrences** across the collection
- **Creating unified index entries** for identical terms
- **Handling variants** (e.g., through stemming: "run," "running" ‚Üí "run")
- **Building comprehensive postings lists** that span the entire collection
- Ensures **efficient storage** and **faster retrieval**

---

#### **Q8. What is the time complexity and space complexity for inverted index?**
**Answer:**  
**Time Complexity:**
- **Building index:** O(n√óm), where n = number of documents, m = average document length
- **Search:** O(k), where k = size of postings list for the term

**Space Complexity:**
- **O(T + P)**, where T = number of unique terms, P = total number of postings entries
- Typically **proportional to collection size**
- With positional information: higher space requirement

---

#### **Q9. What are applications of inverted file?**
**Answer:**  
- **Web search engines:** Google, Bing, Yahoo
- **Enterprise search:** SharePoint, Elasticsearch
- **E-commerce platforms:** Amazon, eBay product search
- **Social media:** Twitter, Facebook search functionality
- **Email systems:** Gmail, Outlook search
- **Code search:** GitHub, GitLab
- **Academic databases:** IEEE Xplore, Google Scholar

---

### **Questions Related to Applications**

#### **Q1. Illustrate the situation when inverted file will not be effective as storage option?**
**Answer:**  
Inverted files are ineffective when:
- **Very small collections:** Overhead of maintaining index exceeds benefits
- **Highly dynamic content:** Frequent updates require constant reindexing
- **Extremely long documents:** Postings lists become too large
- **Non-textual data:** Images, audio, video without text metadata
- **Single-use queries:** Index built but used only once
- **Real-time streaming data:** No time to build and maintain index
- **Memory constraints:** Index size exceeds available storage
- **Unique terms per document:** No term sharing between documents makes inverted index inefficient

---

## üìå Assignment 4: Precision and Recall

### **Frequently Asked Questions**

#### **Q1. What do you mean by Precision?**
**Answer:**  
**Precision** is a performance metric that measures the **accuracy of retrieved results**. It is the ratio of relevant documents retrieved to the total number of documents retrieved.

**Formula:**  
```
Precision = (Number of Relevant Documents Retrieved) / (Total Documents Retrieved)
```

**Example:**  
If 100 documents are retrieved and 70 are relevant:  
Precision = 70/100 = 0.70 or 70%

High precision means fewer irrelevant results.

---

#### **Q2. What do you mean by Recall?**
**Answer:**  
**Recall** is a performance metric that measures the **completeness of retrieved results**. It is the ratio of relevant documents retrieved to the total number of relevant documents in the collection.

**Formula:**  
```
Recall = (Number of Relevant Documents Retrieved) / (Total Relevant Documents in Collection)
```

**Example:**  
If there are 150 relevant documents in collection and 70 are retrieved:  
Recall = 70/150 = 0.467 or 46.7%

High recall means fewer relevant documents are missed.

---

#### **Q3. Explain the need of performance evaluation of IR system**
**Answer:**  
Performance evaluation is necessary to:
- **Measure effectiveness:** Assess how well the system meets user needs
- **Compare systems:** Evaluate different IR algorithms and techniques
- **Identify weaknesses:** Find areas requiring improvement
- **Optimize parameters:** Tune system settings for better performance
- **Justify decisions:** Provide evidence for design choices
- **User satisfaction:** Ensure the system delivers relevant results
- **Business value:** Measure ROI and system impact
- **Continuous improvement:** Track progress over time

---

#### **Q4. Explain how to evaluate performance of IR system.**
**Answer:**  
**Evaluation Methods:**

1. **Test Collection Approach:**
   - Prepare document collection
   - Create query set
   - Establish relevance judgments (ground truth)
   - Run queries and collect results

2. **Calculate Metrics:**
   - **Precision:** Accuracy of results
   - **Recall:** Completeness of results
   - **F-measure:** Harmonic mean of precision and recall
   - **MAP:** Mean Average Precision
   - **NDCG:** Normalized Discounted Cumulative Gain

3. **User Studies:**
   - Conduct usability testing
   - Measure user satisfaction
   - Track click-through rates
   - Monitor query reformulation rates

4. **A/B Testing:**
   - Compare two system versions
   - Measure performance differences
   - Deploy better-performing version

5. **Standard Benchmarks:**
   - Use TREC (Text REtrieval Conference) datasets
   - CLEF (Conference and Labs of the Evaluation Forum)
   - NTCIR (NII Testbeds and Community for Information access Research)

---

## üìå Assignment 5: F-Measure and E-Measure

### **Frequently Asked Questions**

#### **Q1. What do you mean by F Measure?**
**Answer:**  
**F-Measure** (also called F1-Score) is the **harmonic mean of Precision and Recall**. It provides a single score that balances both metrics, giving equal weight to precision and recall.

**Formula:**  
```
F-Measure = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

**Example:**  
If Precision = 0.8 and Recall = 0.6:  
F-Measure = 2 √ó (0.8 √ó 0.6) / (0.8 + 0.6) = 0.686 or 68.6%

**Characteristics:**
- Range: 0 to 1 (higher is better)
- Useful when you need balance between precision and recall
- Penalizes extreme values (very high precision but low recall, or vice versa)

---

#### **Q2. What do you mean by E Measure?**
**Answer:**  
**E-Measure** (also called **FŒ≤-Measure**) is a **weighted harmonic mean** of Precision and Recall that allows adjusting the relative importance of precision versus recall using a parameter Œ≤.

**Formula:**  
```
E-Measure = (1 + Œ≤¬≤) √ó (Precision √ó Recall) / (Œ≤¬≤ √ó Precision + Recall)
```

**Where:**
- **Œ≤ < 1:** Emphasizes Precision (precision is more important)
- **Œ≤ = 1:** Equal weight (becomes standard F-Measure)
- **Œ≤ > 1:** Emphasizes Recall (recall is more important)

**Common variants:**
- **F‚ÇÄ.‚ÇÖ:** Weights precision twice as much as recall
- **F‚ÇÅ:** Standard F-measure (equal weights)
- **F‚ÇÇ:** Weights recall twice as much as precision

---

#### **Q3. What is the Significance of F and E measure in IR system?**
**Answer:**  
**Significance of F-Measure and E-Measure:**

1. **Single Performance Score:**
   - Combines precision and recall into one metric
   - Easier to compare different systems
   - Simplifies reporting and decision-making

2. **Balances Trade-offs:**
   - Addresses the inverse relationship between precision and recall
   - Prevents optimizing one metric at the expense of the other
   - Provides realistic performance assessment

3. **Flexibility (E-Measure):**
   - Allows customization based on application needs
   - Can prioritize precision for critical applications (medical diagnosis)
   - Can prioritize recall for comprehensive searches (legal research)

4. **Application-Specific Tuning:**
   - **E-commerce:** Higher precision (Œ≤ < 1) to avoid showing irrelevant products
   - **Medical research:** Higher recall (Œ≤ > 1) to not miss important studies
   - **General search:** Balanced approach (Œ≤ = 1)

5. **Evaluation Standard:**
   - Widely accepted in IR research
   - Enables comparison across different studies
   - Used in benchmark evaluations (TREC, CLEF)

6. **Quality Indicator:**
   - Low F/E measure indicates system needs improvement
   - High F/E measure suggests effective retrieval
   - Helps identify optimal operating points

---

## üìå Assignment 6: Image Processing & Histograms

### **Frequently Asked Questions**

#### **Q1. What do you mean by image histogram?**
**Answer:**  
An **image histogram** is a graphical representation of the **distribution of pixel intensities** in an image. It shows:
- **X-axis:** Pixel intensity values (0-255 for 8-bit images)
- **Y-axis:** Frequency (number of pixels) at each intensity level
- Provides statistical information about image brightness and contrast
- Helps in image analysis and enhancement

---

#### **Q2. What are different types of file format for images?**
**Answer:**  
Common image file formats:

**Raster Formats:**
- **JPEG (.jpg, .jpeg):** Lossy compression, good for photographs
- **PNG (.png):** Lossless compression, supports transparency
- **GIF (.gif):** Limited colors (256), supports animation
- **BMP (.bmp):** Uncompressed, large file size
- **TIFF (.tif, .tiff):** High quality, used in professional photography
- **WebP:** Modern format, better compression than JPEG/PNG

**Vector Formats:**
- **SVG (.svg):** Scalable vector graphics
- **AI (.ai):** Adobe Illustrator format
- **EPS (.eps):** Encapsulated PostScript

**RAW Formats:**
- **CR2, NEF, ARW:** Camera-specific raw formats

---

#### **Q3. Explain the role of color model**
**Answer:**  
**Role of Color Models:**

1. **Representation:** Define how colors are encoded and stored digitally

2. **Processing:** Enable color manipulation and image processing operations

3. **Display:** Convert digital color data for screen display

4. **Consistency:** Ensure colors appear consistent across devices

5. **Efficiency:** Optimize storage and transmission of color information

6. **Application-specific:** Different models suit different applications:
   - **RGB:** Display on screens
   - **CMYK:** Printing
   - **HSV:** User-friendly color selection
   - **YCbCr:** Video compression

---

#### **Q4. Compare different color models.**
**Answer:**  

| Color Model | Components | Range | Applications | Advantages | Disadvantages |
|-------------|------------|-------|--------------|------------|---------------|
| **RGB** | Red, Green, Blue | 0-255 each | Monitors, cameras, web | Additive, device-oriented | Not perceptually uniform |
| **CMYK** | Cyan, Magenta, Yellow, Black | 0-100% each | Printing | Subtractive, print-oriented | Device-dependent |
| **HSV/HSB** | Hue, Saturation, Value | H:0-360¬∞, S:0-100%, V:0-100% | Color selection, image editing | Intuitive for humans | Complex conversions |
| **YCbCr** | Luminance, Blue-diff, Red-diff | Y:0-255, Cb/Cr:0-255 | Video compression (JPEG, MPEG) | Separates brightness from color | Not intuitive |
| **LAB** | Lightness, A (green-red), B (blue-yellow) | L:0-100, a:-128 to 127, b:-128 to 127 | Color correction, professional editing | Perceptually uniform, device-independent | Complex calculations |
| **Grayscale** | Intensity | 0-255 | Black & white images, preprocessing | Simple, smaller file size | No color information |

---

#### **Q5. Define image histogram?**
**Answer:**  
An **image histogram** is defined as:
- A statistical graph showing the **frequency distribution** of pixel intensity values
- Represents the number of pixels at each gray level or color intensity
- For grayscale: Single histogram with 256 bins (0-255)
- For color: Three separate histograms for R, G, B channels
- Mathematical representation: H(i) = number of pixels with intensity i

**Properties:**
- Sum of all histogram values = total pixels in image
- Provides insights into image contrast, brightness, and dynamic range
- Independent of spatial information (doesn't show where pixels are located)

---

#### **Q6. List types of histogram**
**Answer:**  
**Types of Histograms:**

1. **Grayscale Histogram:**
   - Single channel intensity distribution
   - 256 bins for 8-bit images

2. **Color Histogram:**
   - Separate histograms for R, G, B channels
   - Combined 3D histogram

3. **Cumulative Histogram:**
   - Shows cumulative count up to each intensity level
   - Used in histogram equalization

4. **Normalized Histogram:**
   - Values expressed as probabilities (sum = 1)
   - Useful for comparison between different-sized images

5. **2D Histogram:**
   - Shows relationship between two channels
   - Example: Red vs Green intensity

6. **Multi-dimensional Histogram:**
   - Combines multiple color channels
   - Used in color-based image retrieval

---

#### **Q7. Explain use of image histogram**
**Answer:**  
**Applications of Image Histogram:**

1. **Image Enhancement:**
   - Histogram equalization to improve contrast
   - Histogram stretching for better dynamic range
   - Brightness adjustment

2. **Image Analysis:**
   - Assess image quality (exposure, contrast)
   - Detect under/over-exposed regions
   - Identify color cast

3. **Image Segmentation:**
   - Thresholding based on histogram valleys
   - Separating foreground from background
   - Object detection

4. **Image Retrieval:**
   - Content-based image search
   - Compare images based on color distribution
   - Similarity measurement

5. **Quality Control:**
   - Medical imaging (X-rays, MRI analysis)
   - Manufacturing defect detection
   - Forensic image analysis

6. **Computer Vision:**
   - Feature extraction
   - Pattern recognition
   - Image classification

---

#### **Q8. How to extract R, G and B content from image**
**Answer:**  
**Methods to Extract RGB Channels:**

**Using Python (OpenCV/PIL):**
```python
import cv2
import numpy as np

# Read image
image = cv2.imread('image.jpg')

# Extract channels (OpenCV uses BGR format)
B, G, R = cv2.split(image)

# Alternative method
R_channel = image[:, :, 2]
G_channel = image[:, :, 1]
B_channel = image[:, :, 0]

# Using PIL
from PIL import Image
img = Image.open('image.jpg')
R, G, B = img.split()
```

**Process:**
1. **Load image** into memory
2. **Access pixel array** (typically 3D array: height √ó width √ó 3)
3. **Separate channels:**
   - Red: Extract 3rd dimension index 0 (or 2 in BGR)
   - Green: Extract 3rd dimension index 1
   - Blue: Extract 3rd dimension index 2 (or 0 in BGR)
4. **Store separately** as individual arrays
5. **Display or process** each channel independently

---

### **Questions Related to Applications**

#### **Q1. Consider the image processing domain and explain various methods of extracting intensity of pixel**
**Answer:**  
**Methods to Extract Pixel Intensity:**

1. **Direct Pixel Access:**
   ```python
   pixel_value = image[row, col]  # Returns [B, G, R] for color
   intensity = pixel_value[channel_index]
   ```

2. **Convert to Grayscale:**
   - **Weighted Average Method:**
     - Gray = 0.299√óR + 0.587√óG + 0.114√óB
     - Accounts for human eye sensitivity
   - **Average Method:**
     - Gray = (R + G + B) / 3
     - Simple but less accurate
   - **Lightness Method:**
     - Gray = (max(R,G,B) + min(R,G,B)) / 2

3. **Channel Separation:**
   ```python
   R_intensity = image[:, :, 0]
   G_intensity = image[:, :, 1]
   B_intensity = image[:, :, 2]
   ```

4. **Histogram Analysis:**
   - Extract histogram for each channel
   - Analyze intensity distribution
   - Find dominant intensities

5. **Region-based Extraction:**
   - Select specific region of interest (ROI)
   - Extract average/median intensity
   - Useful for feature extraction

6. **Pixel-by-Pixel Iteration:**
   ```python
   for row in range(height):
       for col in range(width):
           pixel = image[row, col]
           intensity = process(pixel)
   ```

7. **Vectorized Operations (NumPy):**
   ```python
   intensity = np.mean(image, axis=2)  # Average across channels
   ```

**Applications:**
- **Medical imaging:** Analyze tissue density
- **Surveillance:** Motion detection through intensity changes
- **Photography:** Exposure and contrast adjustment
- **Quality control:** Defect detection in manufacturing
- **Remote sensing:** Terrain and vegetation analysis

---

## üìå Assignment 7: Web Crawlers

### **Frequently Asked Questions**

#### **Q1. What is meant by crawler?**
**Answer:**  
A **web crawler** (also called spider, bot, or web robot) is an automated program that:
- **Systematically browses** the World Wide Web
- **Downloads and indexes** web pages
- **Follows hyperlinks** to discover new pages
- **Collects data** for search engines or data analysis
- Operates continuously to keep indexes up-to-date

**Key characteristics:**
- Automated and autonomous
- Follows predefined rules (robots.txt)
- Scalable to handle billions of pages
- Polite (respects crawl delays)

---

#### **Q2. Explain working of web crawler**
**Answer:**  
**Web Crawler Working Process:**

**Step-by-Step Workflow:**

1. **Initialization:**
   - Start with seed URLs (initial set of web pages)
   - Add URLs to a queue

2. **URL Selection:**
   - Pick URL from queue (FIFO or priority-based)
   - Check if URL has been visited before

3. **Fetch Page:**
   - Send HTTP request to web server
   - Download HTML content
   - Handle errors (404, timeouts)

4. **Parse Content:**
   - Extract text, metadata, and links
   - Parse HTML structure
   - Identify outgoing hyperlinks

5. **Process Data:**
   - Index page content
   - Store in database
   - Analyze and extract relevant information

6. **Extract Links:**
   - Find all hyperlinks (<a href="...">)
   - Convert relative URLs to absolute URLs
   - Filter and validate links

7. **Add to Queue:**
   - Add new URLs to crawl queue
   - Avoid duplicates
   - Apply crawling rules

8. **Repeat:**
   - Continue until queue is empty or limit reached
   - Implement politeness policy (delays between requests)

**Architecture Components:**
- **URL Frontier:** Queue of URLs to be crawled
- **Fetcher:** Downloads web pages
- **Parser:** Extracts links and content
- **Indexer:** Stores and indexes content
- **Duplicate Detector:** Avoids re-crawling

---

#### **Q3. Give examples of Web Crawlers.**
**Answer:**  
**Popular Web Crawlers:**

1. **Googlebot:**
   - Google's web crawler
   - Most comprehensive crawler
   - Powers Google Search

2. **Bingbot:**
   - Microsoft's crawler
   - Used for Bing search engine

3. **Slurp Bot:**
   - Yahoo's web crawler
   - Indexes content for Yahoo search

4. **DuckDuckBot:**
   - DuckDuckGo's crawler
   - Privacy-focused search

5. **Baiduspider:**
   - Baidu's crawler (Chinese search engine)
   - Dominant in China

**Open-Source Crawlers:**
- **Scrapy:** Python framework for web scraping
- **Apache Nutch:** Scalable open-source crawler
- **Heritrix:** Internet Archive's crawler
- **StormCrawler:** Apache Storm-based crawler

**Specialized Crawlers:**
- **Price crawlers:** Monitor e-commerce prices
- **News crawlers:** Aggregate news articles
- **Social media crawlers:** Monitor social platforms
- **Academic crawlers:** Index research papers

---

#### **Q4. Specify role of crawler in searching**
**Answer:**  
**Crawler's Role in Search Engines:**

1. **Discovery:**
   - Find new web pages on the internet
   - Detect newly published content
   - Expand search engine coverage

2. **Indexing:**
   - Download and store web page content
   - Create searchable index of web content
   - Update existing indexes with fresh content

3. **Freshness:**
   - Re-crawl pages to detect updates
   - Keep search results current
   - Remove dead or moved pages

4. **Ranking:**
   - Collect signals for ranking algorithms (links, content quality)
   - Analyze page relationships and authority
   - Feed data to search ranking systems

5. **Coverage:**
   - Ensure comprehensive web coverage
   - Prioritize important pages
   - Balance breadth and depth of crawling

**Without crawlers:**
- Search engines couldn't discover content
- Indexes would become outdated
- No automated way to track web changes

---

#### **Q5. Explain Meta Crawler**
**Answer:**  
A **Meta Crawler** (or Meta Search Engine) is:

**Definition:**
- Doesn't maintain its own index of web pages
- Sends queries to multiple search engines simultaneously
- Aggregates and combines results from different sources
- Presents unified results to users

**Working Process:**
1. User submits query to meta crawler
2. Query forwarded to multiple search engines (Google, Bing, Yahoo)
3. Results collected from all sources
4. Duplicates removed
5. Results merged and ranked
6. Unified results displayed to user

**Examples:**
- **Dogpile:** Searches Google, Yahoo, Bing
- **Metacrawler.com:** Aggregates multiple sources
- **Search.com:** Combines various search engines
- **Startpage:** Privacy-focused meta search

**Advantages:**
- Broader coverage (combines multiple indexes)
- Saves time (one query ‚Üí multiple engines)
- Comparison of results from different sources
- No search engine bias

**Disadvantages:**
- Slower response time
- Dependent on other search engines
- Limited control over ranking
- May miss engine-specific features

---

### **Questions Related to Applications**

#### **Q1. Modify developed WebCrawler considering other parameters or keywords**
**Answer:**  
**Enhanced Web Crawler Parameters:**

**1. Crawl Depth Control:**
```python
max_depth = 3  # Limit how deep to follow links
current_depth = 0
```

**2. Domain Restrictions:**
```python
allowed_domains = ['example.com', 'trusted-site.com']
blocked_domains = ['spam-site.com']
```

**3. Content Filtering:**
```python
keywords = ['machine learning', 'data science', 'AI']
min_word_count = 100
language = 'en'
```

**4. Politeness Parameters:**
```python
crawl_delay = 2  # seconds between requests
user_agent = 'MyBot/1.0'
respect_robots_txt = True
```

**5. File Type Filters:**
```python
allowed_extensions = ['.html', '.php', '.asp']
excluded_extensions = ['.pdf', '.jpg', '.zip']
```

**6. Priority Crawling:**
```python
priority_urls = ['homepage', 'sitemap']
priority_keywords = ['latest news', 'updates']
```

**7. Time-based Parameters:**
```python
max_crawl_time = 3600  # 1 hour
last_modified_after = '2024-01-01'
```

**8. Data Extraction Rules:**
```python
extract_fields = ['title', 'meta_description', 'headings', 'images']
store_format = 'json'  # or 'csv', 'database'
```

**9. Duplicate Detection:**
```python
check_url_duplicates = True
check_content_similarity = True
similarity_threshold = 0.9
```

**10. Error Handling:**
```python
max_retries = 3
timeout = 30  # seconds
handle_redirects = True
max_redirects = 5
```

**Modified Crawler Example:**
```python
class AdvancedCrawler:
    def __init__(self, seed_urls, max_depth=3, keywords=None):
        self.seed_urls = seed_urls
        self.max_depth = max_depth
        self.keywords = keywords or []
        self.visited = set()
        self.crawl_delay = 2
        
    def crawl(self, url, depth=0):
        if depth > self.max_depth or url in self.visited:
            return
        
        # Respect politeness
        time.sleep(self.crawl_delay)
        
        # Fetch page
        page = self.fetch_page(url)
        
        # Filter by keywords
        if self.keywords and not self.contains_keywords(page):
            return
        
        # Process and store
        self.process_page(page)
        self.visited.add(url)
        
        # Extract and follow links
        links = self.extract_links(page)
        for link in links:
            self.crawl(link, depth + 1)
```

---

## üìå Assignment 8: Weather API Integration

### **Frequently Asked Questions**

#### **Q1. Explain the steps to implement this assignment?**
**Answer:**  
**Implementation Steps for Weather Prediction:**

**Step 1: Setup and Installation**
```python
# Install required libraries
pip install requests
pip install json
```

**Step 2: Obtain API Key**
- Register at OpenWeatherMap.org
- Generate free API key from account dashboard
- Note the API key for use in code

**Step 3: Import Libraries**
```python
import requests
import json
from datetime import datetime
```

**Step 4: Define API Endpoint**
```python
base_url = "http://api.openweathermap.org/data/2.5/weather"
api_key = "your_api_key_here"
city = "Mumbai"
```

**Step 5: Build Request URL**
```python
complete_url = f"{base_url}?q={city}&appid={api_key}&units=metric"
```

**Step 6: Make API Request**
```python
response = requests.get(complete_url)
data = response.json()
```

**Step 7: Parse JSON Response**
```python
if data["cod"] != "404":
    main = data["main"]
    weather = data["weather"][0]
    
    temperature = main["temp"]
    pressure = main["pressure"]
    humidity = main["humidity"]
    description = weather["description"]
```

**Step 8: Display Results**
```python
print(f"Temperature: {temperature}¬∞C")
print(f"Humidity: {humidity}%")
print(f"Pressure: {pressure} hPa")
print(f"Description: {description}")
```

**Step 9: Error Handling**
```python
try:
    response = requests.get(complete_url)
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"Error: {e}")
```

**Step 10: Save Data (Optional)**
```python
with open('weather_data.json', 'w') as f:
    json.dump(data, f, indent=4)
```

---

#### **Q2. What is OpenWeatherMap service?**
**Answer:**  
**OpenWeatherMap** is:

**Description:**
- A web service providing weather data via APIs
- Offers current weather, forecasts, and historical data
- Covers 200,000+ cities worldwide
- Free and paid tiers available

**Features:**
1. **Current Weather Data:**
   - Real-time temperature, humidity, pressure
   - Wind speed and direction
   - Cloud coverage and visibility
   - Sunrise/sunset times

2. **Weather Forecasts:**
   - 5-day / 3-hour forecast
   - 16-day daily forecast
   - Hourly forecast (48 hours)

3. **Historical Data:**
   - Past weather information
   - Climate statistics

4. **Weather Maps:**
   - Precipitation maps
   - Temperature maps
   - Wind maps
   - Cloud coverage

**API Types:**
- **Free Tier:** 60 calls/minute, basic features
- **Paid Tiers:** Higher limits, advanced features

**Data Format:**
- JSON or XML format
- RESTful API architecture
- Easy integration with any programming language

**Use Cases:**
- Weather applications
- Agriculture planning
- Travel planning
- Event management
- IoT weather stations
- Research and analysis

---

#### **Q3. What is the use of JSON in this assignment?**
**Answer:**  
**Role of JSON in Weather Assignment:**

**1. Data Exchange Format:**
- OpenWeatherMap API returns data in JSON format
- Lightweight and human-readable
- Easy to parse and process

**2. Structured Data Representation:**
```json
{
  "coord": {"lon": 72.85, "lat": 19.01},
  "weather": [
    {
      "id": 721,
      "main": "Haze",
      "description": "haze"
    }
  ],
  "main": {
    "temp": 28.5,
    "feels_like": 30.2,
    "humidity": 65,
    "pressure": 1013
  }
}
```

**3. Easy Parsing in Python:**
```python
import json

# Parse JSON string to Python dictionary
data = json.loads(response.text)

# Access nested data easily
temperature = data['main']['temp']
humidity = data['main']['humidity']
```

**4. Data Storage:**
```python
# Save weather data to file
with open('weather.json', 'w') as f:
    json.dump(data, f, indent=4)

# Load saved data
with open('weather.json', 'r') as f:
    saved_data = json.load(f)
```

**5. Language-Independent:**
- JSON works across all programming languages
- JavaScript, Python, Java, C# all support JSON
- Facilitates cross-platform integration

**Advantages in This Assignment:**
- **Simple parsing:** Direct conversion to Python dictionary
- **Nested structure:** Weather data has hierarchical structure
- **Readability:** Easy to understand and debug
- **Standardized:** Industry-standard format for APIs
- **Flexible:** Can handle varying data structures

---

### **Questions Related to Applications**

#### **Q1. How to use this python code to predict the weather for next few days?**
**Answer:**  
**Implementing Multi-Day Weather Forecast:**

**Step 1: Use Forecast API Endpoint**
```python
# Instead of current weather, use forecast endpoint
forecast_url = "http://api.openweathermap.org/data/2.5/forecast"
complete_url = f"{forecast_url}?q={city}&appid={api_key}&units=metric"
```

**Step 2: Make API Request**
```python
import requests
from datetime import datetime

response = requests.get(complete_url)
forecast_data = response.json()
```

**Step 3: Parse Forecast Data**
```python
if forecast_data["cod"] != "404":
    forecast_list = forecast_data["list"]
    
    # Forecast contains data for every 3 hours
    for item in forecast_list[:8]:  # Next 24 hours (8 x 3-hour intervals)
        dt = datetime.fromtimestamp(item["dt"])
        temp = item["main"]["temp"]
        description = item["weather"][0]["description"]
        
        print(f"{dt}: {temp}¬∞C - {description}")
```

**Step 4: Aggregate Daily Forecasts**
```python
from collections import defaultdict

daily_forecasts = defaultdict(list)

for item in forecast_list:
    date = datetime.fromtimestamp(item["dt"]).date()
    daily_forecasts[date].append(item)

# Calculate daily averages
for date, forecasts in list(daily_forecasts.items())[:5]:  # 5 days
    temps = [f["main"]["temp"] for f in forecasts]
    avg_temp = sum(temps) / len(temps)
    descriptions = [f["weather"][0]["description"] for f in forecasts]
    
    print(f"\n{date}:")
    print(f"  Average Temperature: {avg_temp:.1f}¬∞C")
    print(f"  Conditions: {', '.join(set(descriptions))}")
```

**Step 5: Enhanced Prediction with Statistics**
```python
def get_extended_forecast(city, days=5):
    """
    Get weather forecast for next few days
    """
    forecast_url = "http://api.openweathermap.org/data/2.5/forecast"
    params = {
        'q': city,
        'appid': api_key,
        'units': 'metric',
        'cnt': days * 8  # 8 data points per day (3-hour intervals)
    }
    
    response = requests.get(forecast_url, params=params)
    data = response.json()
    
    daily_summary = {}
    
    for item in data['list']:
        date = datetime.fromtimestamp(item['dt']).strftime('%Y-%m-%d')
        
        if date not in daily_summary:
            daily_summary[date] = {
                'temps': [],
                'humidity': [],
                'descriptions': [],
                'wind_speed': []
            }
        
        daily_summary[date]['temps'].append(item['main']['temp'])
        daily_summary[date]['humidity'].append(item['main']['humidity'])
        daily_summary[date]['descriptions'].append(item['weather'][0]['description'])
        daily_summary[date]['wind_speed'].append(item['wind']['speed'])
    
    # Display forecast
    print(f"\n5-Day Weather Forecast for {city}\n")
    print("=" * 70)
    
    for date, data in daily_summary.items():
        avg_temp = sum(data['temps']) / len(data['temps'])
        max_temp = max(data['temps'])
        min_temp = min(data['temps'])
        avg_humidity = sum(data['humidity']) / len(data['humidity'])
        avg_wind = sum(data['wind_speed']) / len(data['wind_speed'])
        common_desc = max(set(data['descriptions']), 
                         key=data['descriptions'].count)
        
        print(f"\n{date}:")
        print(f"  Temperature: {avg_temp:.1f}¬∞C (Min: {min_temp:.1f}¬∞C, Max: {max_temp:.1f}¬∞C)")
        print(f"  Conditions: {common_desc}")
        print(f"  Humidity: {avg_humidity:.0f}%")
        print(f"  Wind Speed: {avg_wind:.1f} m/s")

# Usage
get_extended_forecast("Mumbai", days=5)
```

**Step 6: Visualization (Optional)**
```python
import matplotlib.pyplot as plt

dates = []
temperatures = []

for date, data in daily_summary.items():
    dates.append(date)
    temperatures.append(sum(data['temps']) / len(data['temps']))

plt.figure(figsize=(10, 6))
plt.plot(dates, temperatures, marker='o')
plt.xlabel('Date')
plt.ylabel('Temperature (¬∞C)')
plt.title('5-Day Temperature Forecast')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()
```

**Key Points:**
- Forecast API provides data in 3-hour intervals
- Aggregate data by day for daily forecasts
- Calculate min, max, and average temperatures
- Handle multiple weather conditions per day
- Free API typically provides 5-day forecast
- Consider using paid tier for 16-day forecast

---

## üìå Assignment 9: Recommender Systems

### **Frequently Asked Questions**

#### **Q1. Define recommender system**
**Answer:**  
A **Recommender System (RS)** is a software application that:
- Analyzes user preferences, behaviors, and interactions
- Predicts items (products, movies, music, doctors) that users might like
- Suggests relevant items to users based on patterns and similarities
- Helps users discover new items of interest

**Formal Definition:**
A system that produces individualized recommendations as output or has the effect of guiding the user in a personalized way to interesting or useful objects in a large space of possible options.

**Key Components:**
- **Input:** User data, item data, interaction history
- **Processing:** Filtering algorithms, machine learning models
- **Output:** Ranked list of recommended items

---

#### **Q2. List types of recommender system.**
**Answer:**  
**Main Types of Recommender Systems:**

1. **Collaborative Filtering (CF)**
   - User-based CF
   - Item-based CF
   - Matrix factorization

2. **Content-Based Filtering (CBF)**
   - Feature-based recommendations
   - Profile matching

3. **Hybrid Recommender Systems**
   - Combines CF and CBF
   - Weighted, switching, or mixed approaches

4. **Knowledge-Based Systems**
   - Case-based reasoning
   - Constraint-based recommendations

5. **Demographic-Based Systems**
   - User demographic information
   - Stereotypical recommendations

6. **Context-Aware Systems**
   - Location-based
   - Time-sensitive
   - Mood-aware

7. **Social Recommender Systems**
   - Friend-based recommendations
   - Social network influence

---

#### **Q3. Explain how the Recommender systems reduce information overload by estimating Relevance?**
**Answer:**  
**How Recommender Systems Reduce Information Overload:**

**1. Problem of Information Overload:**
- Modern platforms have millions of items (products, movies, articles)
- Users cannot browse or evaluate everything
- Too many choices lead to decision paralysis
- Time wasted searching for relevant items

**2. Relevance Estimation Methods:**

**A. Collaborative Filtering:**
- Analyzes behavior of similar users
- Estimates: "Users like you preferred these items"
- Filters millions of items to top-N relevant ones
- Example: If User A and B rated 80% movies similarly, items liked by A are relevant to B

**B. Content-Based Filtering:**
- Matches item features with user preferences
- Estimates: "This item is similar to what you liked before"
- Reduces search space to similar items only
- Example: If you liked action movies, recommend other action movies

**C. Popularity and Trending:**
- Leverages collective wisdom
- Estimates: "Many users found this relevant recently"
- Highlights items with high engagement

**D. Personalization:**
- Creates user profiles from past interactions
- Estimates relevance based on individual preferences
- Filters out irrelevant categories entirely

**3. Practical Impact:**

**Without Recommender:**
- Netflix: Browse 15,000+ titles manually
- Amazon: Search millions of products
- Spotify: Explore 100+ million songs

**With Recommender:**
- Personalized homepage with 20-50 relevant items
- 80% time saved in finding relevant content
- Higher satisfaction and engagement

**4. Relevance Estimation Techniques:**

**Explicit Signals:**
- User ratings (5-star systems)
- Likes/dislikes
- Reviews and comments
- Wish lists

**Implicit Signals:**
- Click-through rates
- Time spent on items
- Purchase history
- Browse patterns
- Cart additions

**5. Ranking and Filtering:**
```
Million Items ‚Üí Relevance Estimation ‚Üí Top 100 Candidates ‚Üí 
Personalized Ranking ‚Üí Top 10 Recommendations
```

**6. Benefits:**
- **For Users:** Find relevant items quickly, discover new interests
- **For Businesses:** Increased sales, better engagement, customer retention
- **For Platforms:** Reduced churn, improved user experience

**Example - Netflix:**
- 80% of watched content comes from recommendations
- Without recommendations, users would face 15,000+ title choices
- Recommendations reduce this to personalized rows of 20-30 titles
- Estimates relevance using viewing history, ratings, and similar users

---

#### **Q4. Explain Collaborative filtering**
**Answer:**  
**Collaborative Filtering (CF) Overview:**

**Definition:**
Collaborative Filtering recommends items based on the preferences and behaviors of similar users. It assumes that users who agreed in the past will agree in the future.

**Core Principle:**
"People who are similar to you liked these items, so you might like them too."

**Types of Collaborative Filtering:**

**1. User-Based Collaborative Filtering:**

**Process:**
1. Find users similar to the target user
2. Identify items liked by similar users
3. Recommend those items to target user

**Example:**
```
User A: Liked Movies [M1, M2, M3, M4]
User B: Liked Movies [M1, M2, M3, M5]
User C: Liked Movies [M1, M2]

User A and B are similar (75% overlap)
Recommend M5 to User A (since similar User B liked it)
```

**Similarity Measures:**
- **Pearson Correlation Coefficient:** Measures linear correlation
- **Cosine Similarity:** Measures angle between user vectors
- **Jaccard Similarity:** Measures set overlap

**2. Item-Based Collaborative Filtering:**

**Process:**
1. Find items similar to items user already liked
2. Calculate item-item similarities
3. Recommend similar items

**Example:**
```
Movie M1 was liked by users [U1, U2, U3, U4]
Movie M2 was liked by users [U1, U2, U3, U5]
M1 and M2 are similar (75% user overlap)

If user likes M1, recommend M2
```

**Advantages:**
- More stable (item relationships change less than user preferences)
- Faster for systems with more users than items
- Better scalability

**3. Matrix Factorization:**

Modern CF uses matrix factorization techniques:
- **SVD (Singular Value Decomposition)**
- **ALS (Alternating Least Squares)**
- **Neural Collaborative Filtering**

**User-Item Rating Matrix:**
```
        Item1  Item2  Item3  Item4
User1     5      4      ?      2
User2     4      ?      3      3
User3     ?      3      4      4
User4     2      2      5      ?
```

**Advantages of Collaborative Filtering:**
1. **No domain knowledge needed:** Works without understanding item content
2. **Serendipity:** Can recommend unexpected but relevant items
3. **Cross-domain:** Same technique works for movies, products, music
4. **Learns patterns:** Discovers complex user preferences automatically
5. **Quality improves:** More data = better recommendations

**Disadvantages:**
1. **Cold Start Problem:**
   - New users have no history
   - New items have no ratings
   - Cannot make recommendations initially

2. **Data Sparsity:**
   - Most users rate only small fraction of items
   - Sparse rating matrix affects accuracy
   - Difficult to find similar users/items

3. **Scalability:**
   - Computationally expensive for large datasets
   - Millions of users √ó millions of items = billions of comparisons

4. **Grey Sheep:**
   - Users with unique tastes hard to match
   - No similar users found

5. **Popularity Bias:**
   - Tends to recommend popular items
   - Niche items get less exposure

**Real-World Applications:**
- **Amazon:** "Customers who bought this also bought"
- **Netflix:** Movie recommendations based on viewing history
- **Spotify:** Playlist recommendations
- **YouTube:** Video suggestions

**Implementation Example:**
```python
from scipy.spatial.distance import cosine

# User-Item matrix (rows=users, cols=items)
ratings = np.array([
    [5, 4, 0, 2],
    [4, 0, 3, 3],
    [0, 3, 4, 4],
    [2, 2, 5, 0]
])

# Calculate similarity between users
user1 = ratings[0]  # [5, 4, 0, 2]
user2 = ratings[1]  # [4, 0, 3, 3]

similarity = 1 - cosine(user1, user2)

# Predict rating for user1 on item3 (currently 0)
# Use weighted average of similar users' ratings
```

---

#### **Q5. Explain Hybrid Recommender system**
**Answer:**  
**Hybrid Recommender System Overview:**

**Definition:**
A Hybrid Recommender System combines multiple recommendation techniques (typically Collaborative Filtering and Content-Based Filtering) to overcome limitations of individual approaches and provide more accurate, robust recommendations.

**Why Hybrid Systems?**

**Problems with Pure CF:**
- Cold start for new users/items
- Data sparsity
- Cannot recommend to users with unique tastes

**Problems with Pure CBF:**
- Limited to recommending similar items
- Requires detailed item features
- Cannot discover unexpected preferences (no serendipity)

**Solution:** Combine both approaches to leverage strengths and minimize weaknesses

**Types of Hybridization:**

**1. Weighted Hybrid:**
- Combine scores from multiple recommenders
- Linear combination with weights

```
Final_Score = w1 √ó CF_Score + w2 √ó CBF_Score + w3 √ó Popularity_Score
Example: Score = 0.6 √ó CF + 0.3 √ó CBF + 0.1 √ó Popularity
```

**2. Switching Hybrid:**
- Switch between techniques based on situation
- Use content-based for new items (cold start)
- Use collaborative for items with sufficient ratings

```
if item_has_enough_ratings:
    use_collaborative_filtering()
else:
    use_content_based_filtering()
```

**3. Mixed Hybrid:**
- Present recommendations from multiple systems simultaneously
- User sees diverse recommendations

```
Recommendations:
- Top 5 from Collaborative Filtering
- Top 3 from Content-Based
- Top 2 from Trending/Popular
```

**4. Feature Combination:**
- Treat collaborative information as additional features
- Feed combined features to single recommender

```
Features = [content_features, user_similarity, item_similarity, ratings]
Model.predict(Features)
```

**5. Cascade Hybrid:**
- Use one technique to refine another
- First system produces candidates
- Second system ranks them

```
Step 1: CF generates 100 candidates
Step 2: CBF ranks these 100 ‚Üí Top 10
```

**6. Feature Augmentation:**
- One technique creates features for another
- CF generates latent features
- CBF uses these features

**7. Meta-Level Hybrid:**
- One model's output becomes another's input
- CBF builds user profile
- CF uses this profile for recommendations

**Advantages of Hybrid Systems:**

1. **Overcomes Cold Start:**
   - Use content-based for new items/users
   - Switch to collaborative as data grows

2. **Improved Accuracy:**
   - Combines multiple signals
   - More robust predictions

3. **Serendipity + Relevance:**
   - CF provides unexpected discoveries
   - CBF ensures relevance

4. **Handles Sparsity:**
   - Content-based works even with sparse ratings
   - CF adds collaborative insights

5. **Explanation:**
   - Can explain recommendations using content features
   - More transparent than pure CF

**Disadvantages:**

1. **Increased Complexity:**
   - More difficult to implement and maintain
   - Multiple models to train and tune

2. **Higher Computational Cost:**
   - Running multiple algorithms
   - Combining results adds overhead

3. **Requires More Data:**
   - Needs both ratings and content features
   - Content metadata must be maintained

**Real-World Examples:**

**Netflix Hybrid Approach:**
- Collaborative filtering for personalization
- Content-based for metadata matching (genre, actors)
- Popularity and trending for new users
- Deep learning models combining all signals

**Amazon:**
- Item-to-item collaborative filtering
- Content-based product features
- Purchase history analysis
- Browsing behavior patterns

**Spotify:**
- Collaborative filtering (user listening patterns)
- Content-based (audio features, lyrics)
- NLP on playlist names and descriptions
- Social features (friend activity)

**Implementation Example:**
```python
class HybridRecommender:
    def __init__(self):
        self.cf_model = CollaborativeFiltering()
        self.cbf_model = ContentBasedFiltering()
        self.weights = {'cf': 0.6, 'cbf': 0.4}
    
    def recommend(self, user_id, item_id):
        # Get scores from both models
        cf_score = self.cf_model.predict(user_id, item_id)
        cbf_score = self.cbf_model.predict(user_id, item_id)
        
        # Weighted combination
        hybrid_score = (self.weights['cf'] * cf_score + 
                       self.weights['cbf'] * cbf_score)
        
        return hybrid_score
    
    def get_recommendations(self, user_id, n=10):
        # Switching strategy for cold start
        if self.is_new_user(user_id):
            # Use content-based for new users
            return self.cbf_model.recommend(user_id, n)
        else:
            # Use hybrid for existing users
            candidates = self.get_candidates(user_id)
            scores = [self.recommend(user_id, item) 
                     for item in candidates]
            top_items = sorted(zip(candidates, scores), 
                             key=lambda x: x[1], reverse=True)
            return [item for item, score in top_items[:n]]
```

**Modern Hybrid Approaches:**
- **Deep Learning:** Neural networks combining multiple inputs
- **Ensemble Methods:** Combine predictions from multiple models
- **Context-Aware:** Add temporal, location, and device context
- **Multi-Armed Bandits:** Balance exploration and exploitation

---

### **Questions Related to Applications**

#### **Q1. If you want to buy a product from Amazon.in website. Explain how recommender system is helpful to enhance your buying product experience**
**Answer:**  
**How Amazon's Recommender System Enhances Shopping Experience:**

**1. Product Discovery Phase:**

**Personalized Homepage:**
- Amazon analyzes your browsing history
- Shows products in categories you're interested in
- Displays "Recommended for You" section
- Saves time searching through millions of products

**Example:** If you frequently buy electronics, homepage highlights latest gadgets rather than books or clothing.

**2. Search Enhancement:**

**Autocomplete Suggestions:**
- Predicts what you're searching for
- Based on your past searches and popular queries
- Helps formulate better search queries

**Filtered Results:**
- Search results ranked by relevance to your preferences
- Items you're likely to buy appear first
- Less relevant items filtered to lower positions

**3. Product Page Recommendations:**

**"Frequently Bought Together":**
- Collaborative filtering at work
- Shows complementary products
- Example: Buying phone ‚Üí suggests phone case, screen protector
- Increases cart value through relevant suggestions

**"Customers Who Bought This Also Bought":**
- Item-based collaborative filtering
- Discovers related products you might need
- Example: Buying camera ‚Üí suggests memory card, camera bag

**"Compare With Similar Items":**
- Content-based filtering
- Shows products with similar features
- Helps make informed decisions
- Example: Compare different laptop models side-by-side

**4. Personalized Recommendations:**

**"Inspired by Your Browsing History":**
- Content-based + collaborative filtering
- Tracks items you viewed but didn't buy
- Reminds you of products you were considering
- Example: Viewed 5 different shoes ‚Üí shows more shoes in similar style

**"Recommended Based on Items You've Viewed":**
- Hybrid recommendation
- Uses both your behavior and similar users
- Discovers new products in your interest areas

**5. Email and Notifications:**

**Personalized Emails:**
- "Products you might like" based on browsing
- Price drop alerts for wishlisted items
- New arrivals in favorite categories
- Back-in-stock notifications

**6. During Checkout:**

**"Add-on Items":**
- Small, relevant products suggested
- Complete your purchase experience
- Example: Buying laptop ‚Üí suggests mouse, laptop sleeve

**"Save for Later" Recommendations:**
- Items similar to what's in your cart
- Alternative products if price is concern

**7. Post-Purchase:**

**"Buy It Again":**
- For frequently purchased items
- Predicts when you might need refills
- Example: Monthly grocery items, consumables

**"Based on Your Recent Purchase":**
- Complementary products for items you bought
- Example: Bought DSLR camera ‚Üí suggests lenses, tripod

**Technical Implementation:**

**Data Collected:**
1. **Explicit Data:**
   - Ratings and reviews you write
   - Wishlist additions
   - Product questions asked
   - Customer service interactions

2. **Implicit Data:**
   - Browsing history (products viewed)
   - Time spent on product pages
   - Items added to cart
   - Purchase history
   - Search queries
   - Click patterns
   - Return/refund patterns

**Recommendation Algorithms Used:**

**Collaborative Filtering:**
```
User A bought [Item1, Item2, Item3]
User B bought [Item1, Item2, Item4]
‚Üí Recommend Item4 to User A
‚Üí Recommend Item3 to User B
```

**Item-to-Item CF:**
```
50% of buyers who purchased "iPhone 15" also bought "iPhone case"
‚Üí When you add iPhone to cart, suggest case
```

**Content-Based:**
```
You bought: "Nike Running Shoes, Size 10, Blue"
‚Üí Recommend: Other Nike shoes, Size 10, Sports shoes, Blue products
```

**Hybrid Approach:**
```
Weight = 0.4 √ó Collaborative + 0.3 √ó Content + 0.2 √ó Popularity + 0.1 √ó Trending
```

**Benefits for Customer:**

1. **Time Savings:**
   - No need to browse thousands of products
   - Relevant products shown immediately
   - Quick discovery of needed items
   - **Example:** Instead of browsing 50,000 books, see 20 personalized recommendations

2. **Better Decisions:**
   - Compare similar products easily
   - See what others who bought also purchased
   - Read relevant reviews
   - **Example:** Comparing 5 similar laptops suggested based on your requirements

3. **Discovery of New Products:**
   - Find products you didn't know existed
   - Discover complementary items
   - Serendipitous recommendations
   - **Example:** Buying coffee maker ‚Üí discover specialty coffee beans you'd love

4. **Personalized Experience:**
   - Feels like shopping in a store with personal assistant
   - Recommendations match your taste and budget
   - No irrelevant spam
   - **Example:** Budget-conscious shopper sees deals; premium buyer sees high-end options

5. **Price Optimization:**
   - See alternative products at different price points
   - Get notified of price drops
   - Compare prices across similar items
   - **Example:** "Similar item available at 30% less with comparable ratings"

6. **Avoiding Choice Paralysis:**
   - Too many options lead to decision fatigue
   - Recommendations narrow down choices
   - Focus on what matters to you
   - **Example:** 10 curated laptop choices instead of 1000

7. **Complete Purchase:**
   - Don't forget essential accessories
   - Bundle deals and savings
   - Complete the solution
   - **Example:** Buying printer ‚Üí reminded to buy ink cartridges

**Real Shopping Scenario:**

**Scenario:** Buying a laptop on Amazon

**Step 1 - Homepage:**
- See "Recommended laptops based on your browsing" (you've been looking at laptops)
- Personalized deals on electronics

**Step 2 - Search:**
- Type "laptop" ‚Üí autocomplete suggests "laptop for programming under 60000"
- Search results ranked by your preferences (RAM, processor priority)

**Step 3 - Product Page:**
- View Dell XPS 15
- See "Frequently bought together" ‚Üí Mouse, laptop bag, screen protector (Bundle discount!)
- "Compare with similar items" ‚Üí 3 similar laptops side-by-side
- "Customers also viewed" ‚Üí HP Pavilion, Lenovo ThinkPad

**Step 4 - Decision:**
- Reviews from users with similar purchase history highlighted
- "Highly rated by customers who bought similar items"
- Video reviews from verified purchasers

**Step 5 - Cart:**
- Added laptop to cart
- "Don't forget" ‚Üí Laptop cooling pad, external hard drive
- "Complete your purchase" ‚Üí Laptop sleeve, HDMI cable

**Step 6 - Checkout:**
- "Customers who bought this also bought" ‚Üí Wireless mouse at special price

**Step 7 - Post-Purchase:**
- Email: "Accessories for your new Dell XPS 15"
- Recommendations: Software, extended warranty, laptop stand
- "Buy it again" suggestion after 2 years for upgrade

**Business Benefits for Amazon:**

1. **Increased Sales:**
   - 35% of Amazon's revenue comes from recommendations
   - Cross-selling and up-selling
   - Higher average order value

2. **Customer Retention:**
   - Better experience = repeat customers
   - Personalization builds loyalty
   - Reduced churn

3. **Inventory Management:**
   - Move slow-selling inventory through recommendations
   - Balance supply and demand
   - Promote high-margin products strategically

4. **Reduced Search Costs:**
   - Users find products faster
   - Less abandoned searches
   - Higher conversion rates

**Technical Stack:**

- **Machine Learning Models:** Neural networks, matrix factorization
- **Real-time Processing:** Stream processing for immediate updates
- **Big Data:** Handles billions of interactions
- **A/B Testing:** Constantly testing new algorithms
- **Personalization Engine:** Individual profile for each user

**Privacy Considerations:**

- Amazon collects extensive data
- Users can manage recommendation preferences
- Option to clear browsing history
- Trade-off: Privacy vs. Personalization

**Conclusion:**

Amazon's recommender system transforms shopping from:
- **Searching** ‚Üí **Discovering**
- **Overwhelming choices** ‚Üí **Curated selections**
- **Generic store** ‚Üí **Personal shopping assistant**
- **Random browsing** ‚Üí **Targeted suggestions**

**Result:** Saves time, improves decisions, increases satisfaction, and ultimately leads to better shopping experience and more purchases.

---

## ‚úÖ Summary

This document covers comprehensive viva questions and answers for all 9 Information Retrieval lab assignments:

1. **Assignment 1:** Information Retrieval basics, conflation, feedback mechanisms
2. **Assignment 2:** Clustering algorithms, similarity measures, single pass algorithm
3. **Assignment 3:** Inverted index, dictionary, postings, word-oriented mechanisms
4. **Assignment 4:** Precision, Recall, IR system performance evaluation
5. **Assignment 5:** F-Measure, E-Measure, significance in IR
6. **Assignment 6:** Image processing, histograms, color models, RGB extraction
7. **Assignment 7:** Web crawlers, working mechanism, meta crawlers
8. **Assignment 8:** Weather API integration, OpenWeatherMap, JSON usage
9. **Assignment 9:** Recommender systems, collaborative filtering, hybrid systems, Amazon case study

Each section includes:
- ‚úÖ Fundamental concepts and definitions
- ‚úÖ Practical applications and examples
- ‚úÖ Real-world use cases
- ‚úÖ Implementation considerations
- ‚úÖ Advantages and disadvantages
- ‚úÖ Technical details and algorithms

**Best of luck with your viva examination! üéì**
